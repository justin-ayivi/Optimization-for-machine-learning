# Optimization-for-machine-learning
Optimization algorithms (SAGA, SAG, RMSProp, Nesterov Accelerated Gradient, Stochastic and Mini-batch Gradient)


The report must as often as possible link the obtained results with the theory of the course. 
Mandatory things to put in the report: 
- Presentation of the dataset. It should be medium scale (~10K points/features), and as original as possible. This part can include some useful visualization (1D display of features, histogramming, pca, t-sne, etc).
- Presentation of the problem. It can be either classification (2 or more classes) or regression.
- Test of batch gradient descent, with exploration of at least the descent step size and link with the course.
- Test of accelerated gradient descent (Nesterov),
- Test of stochastic gradient descent (impact of hyperparameters such as learning rate, (mini)-batch size, diagonal scaling, etc)
- Test of regularization (ridge/lasso using iterative soft thresholding)
